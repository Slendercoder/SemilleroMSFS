{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<table><tr>\n",
    "    <td><img src=\"imagenes/Macc.png\"/></td>\n",
    "    <td>Semillero de Modelación y Simulación de Fenómenos Sociales</td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rhcWf_uIbAsK"
   },
   "source": [
    "# Simulación del Modelo del bar El Farol\n",
    "\n",
    "## Descripción del modelo\n",
    "\n",
    "En el presente modelo se tiene una población de $N$ individuos, quienes deben tomar la decisión de si acudirán o no al bar llamado \"El Farol\". Cada agente toma esta decisión de manera independiente a los demás, y se realiza de manera simultánea. \n",
    "\n",
    "<table><tr>\n",
    "    <td><img src=\"imagenes/ElFarol1.jpeg\"/></td>\n",
    "    <td><img src=\"imagenes/ElFarol2.jpeg\"/></td>\n",
    "</tr></table>\n",
    "\n",
    "Si el total de acudientes supera un umbral $\\mu$, entonces El Farol se congestionará y los individuos que hayan ido al bar no tendrán una buena experiencia (recompensa=-1). Por otro lado, si el total de los acudientes no supera este umbral, quienes hayan asistido podrán disfrutar su estadía en el bar (recompensa=1). Adicionalmente, si un individuo no asiste al bar, será indiferente al estado de El Farol (recompensa=0). Cada agente intentará tomar la mejor decisión posible con respecto a asistir o no al bar. Esta situación se repetirá por un número $k$ de iteraciones. \n",
    "\n",
    "## Suposiciones de nuestra simulación\n",
    "\n",
    "**Políticas**\n",
    "\n",
    "Los agentes que simularemos tendrán memoria de 1 iteración. Es decir, el agente recordará la decisión que tomó en la iteración inmediatamente anterior (estrategia) así como la ganancia que obtuvo (recompensa). Cada individuo empleará de manera diferente esta información, siguiendo diferentes políticas. En nuestro modelo sólo hay tres posibles combinaciones de estrategias y recompensas, a saber:\n",
    "\n",
    "(no ir a El Farol, recompensa 0) \n",
    "(ir a El Farol, recompensa 1) \n",
    "(ir a El Farol, recompensa -1)\n",
    "\n",
    "Estas combinaciones se codifican de la siguiente manera, teniendo en cuenta que no ir al bar es 0 e ir es 1:\n",
    "\n",
    "(0, 0)  \n",
    "(1, 1)   \n",
    "(1, -1)\n",
    "\n",
    "Para cada combinación, el agente tiene dos opciones en la ronda $n+1$: ir o no ir. Esto nos da ocho políticas posibles.\n",
    "\n",
    "**Aprendizaje**\n",
    "\n",
    "Asumimos que los agentes están conectados mediante una red social. Esta red determina la vecindad de cada agente. Después de cada ronda, los agentes revisan la ganancia recibida en su vecindad y adoptan la política del vecino con mayor puntaje. Si nadie tiene un mejor puntaje, continúan con la política que tenían."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación en Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random as rd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import AnnotationBbox, OffsetImage\n",
    "import redes\n",
    "import ElFarolFunciones as F\n",
    "import seaborn as sns\n",
    "import redes1\n",
    "import copy\n",
    "import igraph as ig\n",
    "\n",
    "# Comando para mostrar gráficos en notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AGENTES \n",
    "Los agentes que se simularán tendrán 4 atributos:\n",
    "\n",
    "**Estado**: Es la estrategia tomada por el agente y que puede ser ir al bar (1) o abstenerse de ir (0).\n",
    "\n",
    "**Puntaje:** Puntaje obtenido. Hay tres posibilidades:\n",
    "\n",
    "- **1** Si fue y la proporción de agentes que fueron fue menor que el $\\mu $ determinado ($\\mu=0.5$).\n",
    "- **0**, si decidió no ir.\n",
    "- **-1**, si fue y la proporción de agentes que fueron fue mayor que el $\\mu$ determinado ($\\mu=0.5$).\n",
    "\n",
    "**Politica**: Es la forma en que los agentes toman la decisión de actuar en la siguiente ronda dada la situación actual, en términos de su estado y puntaje.\n",
    "\n",
    "**Vecinos:** Los vecinos son los agentes conectados mediante la red. Son los únicos sobre los que un agente puede tener información, en términos de puntaje y política.\n",
    "\n",
    "La impementación en Python de estos atributos tiene la característica siguiente. Cada atributo se almacena como una lista, la cual guarda la información correspondiente por cada ronda. Esto es, por ejemplo, `agente.estado[0]` es el estado del agente en la primera ronda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I2YA5E0BbIBZ"
   },
   "outputs": [],
   "source": [
    "class agente:\n",
    "    def __init__(self, estados, scores, politicas, vecinos):\n",
    "        self.estado = estados # lista\n",
    "        self.score = scores # lista\n",
    "        self.politica = politicas # lista\n",
    "        self.vecinos = vecinos\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"E:{0}, S:{1}, P:{2}, V{3}\".format(self.estado, self.score,self.politica,self.vecinos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wJ39jjcYbI1V"
   },
   "source": [
    "Utilizando la clase **agente** podemos generar agentes aleatorios creando objetos de esta clase a los que se les asigna un estado y una política al azar. Una vez conocido el estado inicial de todos los agentes, se procede a calcular la proporción de agentes que fueron al bar, mediante la función `calculamedio()`. Dependiendo de este resultado, se le asigna un puntaje a cada agente. Así pues, esta función recibe como parámetro el número de agentes que se quieren crear y retorna una lista con los agentes nuevos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-3uUM9QUbK3-"
   },
   "outputs": [],
   "source": [
    "def crear_agentes_aleatorios(Num_agentes):\n",
    "    Agentes = []\n",
    "    for i in range(Num_agentes):\n",
    "        Agentes.append(agente([rd.randint(0,1)], [], [rd.randint(0,7)], []))\n",
    "\n",
    "    X = F.calcula_medio(Agentes)\n",
    "    \n",
    "    for a in Agentes:\n",
    "        if a.estado[-1] == 1:\n",
    "            if X > 0.5:\n",
    "                a.score.append(-1)\n",
    "            else:\n",
    "                a.score.append(1)\n",
    "        else:\n",
    "            a.score.append(0)\n",
    "\n",
    "    return Agentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1YlaeeLDbLkM"
   },
   "source": [
    "Recordemos que las políticas representan la toma de decisiones de los agentes de la ronda $n$ a la ronda $n+1$. Para ello, cada agente consulta tanto la estrategia de la ronda $n$, como la recompensa obtenida. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "73I5YcbubMeJ"
   },
   "outputs": [],
   "source": [
    "def crear_politicas():\n",
    "    politicas = [\n",
    "    {(0,0): 0, (1,1): 0, (1, -1): 0}, #0\n",
    "    {(0,0): 0, (1,1): 0, (1, -1): 1}, #1\n",
    "    {(0,0): 0, (1,1): 1, (1, -1): 0}, #2\n",
    "    {(0,0): 0, (1,1): 1, (1, -1): 1}, #3\n",
    "    {(0,0): 1, (1,1): 0, (1, -1): 0}, #4\n",
    "    {(0,0): 1, (1,1): 0, (1, -1): 1}, #5\n",
    "    {(0,0): 1, (1,1): 1, (1, -1): 0}, #6\n",
    "    {(0,0): 1, (1,1): 1, (1, -1): 1}, #7\n",
    "    ]\n",
    "    return politicas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si queremos que haya más interacción entre los agentes, se utiliza la \n",
    "función `agentes_aprenden` la cual recibe como parámetro la lista de\n",
    "agentes y compara el resultado de la última ronda con el de sus vecinos.\n",
    "Cada agente se acoge a la política del vecino con mayor puntaje, si nadie tiene\n",
    "un mejor puntaje continúa con la política que tenía."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agentes_aprenden(Agentes, ronda):   \n",
    "    #Los agentes copian la politica del ganador de la Ronda\n",
    "    for agente in Agentes:\n",
    "        #print(Agentes.index(agente))\n",
    "        maximo=agente.score[ronda]\n",
    "        maximo_vecino=Agentes.index(agente)\n",
    "        #print(agente.vecinos)\n",
    "        for index_vecino in agente.vecinos:\n",
    "            if((Agentes[index_vecino].score[ronda])>(maximo)):\n",
    "                #print('Hay cambio')\n",
    "                #print('Puntaje anterior',maximo)\n",
    "                maximo=Agentes[index_vecino].score[ronda]\n",
    "                #print('Puntaje anterior vecino',maximo)\n",
    "                maximo_vecino=index_vecino\n",
    "            else: print('')\n",
    "                #print('No hay cambio')\n",
    "        agente.politica.append(Agentes[maximo_vecino].politica[ronda])\n",
    "    return Agentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con las funciones anteriores, estamos listos para hacer la simulación, definiendo los parámetros iniciales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Num_agentes = 7\n",
    "Num_iteraciones = 50\n",
    "TIPO_RED = 0 # CAMBIE ESTE VALOR PARA VER DIFERENTES TIPOS DE REDES\n",
    "PARS = [Num_agentes, 0.3]\n",
    "# Generando red a archivo\n",
    "if TIPO_RED == 0:\n",
    "#    redes.random_graph(*PARS)\n",
    "    redes1.create_graph(PARS[0], \"GRG\", PARS[1], True)    \n",
    "elif TIPO_RED == 1:\n",
    "    redes.small_world(*PARS)\n",
    "elif TIPO_RED == 2:\n",
    "    redes.scale_free(PARS[0], PARS[0]/4, PARS[0]/8, PARS[1]*0.6, PARS[1])\n",
    "elif TIPO_RED == 3:    \n",
    "    redes1.create_graph(Num_agentes, \"CloseRing\",0,True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corra esta celda para ver el grafo generado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0, 0.0, 1.0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASUAAAEeCAYAAADM2gMZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAANF0lEQVR4nO3df2zU9R3H8df3ri1dy7VCK7QkWoRgUTAQs1o0GkXRTDcnMZlEpjMDCTPKFE0ExIFakU1w6AwODVtYpkyXEeo6MsCxgDoQHZkCgmi1QoXKldJf9Bft3e0PXQfSFqjt9/O+u+fjP3vEe0Xh2U+/d9zXi8ViAgArAq4HAMCJiBIAU4gSAFOIEgBTiBIAU4gSAFOIEgBTiBIAU4gSAFOIEgBTiBIAU4gSAFOIEgBTiBIAU4gSAFOIEgBTiBIAU4gSAFOIEgBTiBIAU4gSAFNSfH4+bp0CxC/PjyfhpATAFKIEwBSilICi0aiOHTumlpYW11OAs0aUEsyCOXP0z7Q0Hc3K0sGBA7UwN1fhw4ddzwLOmOfzbbu50N2PfnbnnXrh5ZdP+U7zy9RU/byuThkZGU52IWH4cqGbKCWIpqYmHQqFNKqb/5+3jRqlP3/8sc+rkGB49Q1nLvzllyro4RvMfZ995uMaoPeIUoI43RE06O+JGOg1opQgzh0yRIe97k/XL1xwgY9rgN4jSgkiFApp4S23KNrFY78LBPT0li2+bwJ6gyglkN+vXat777pLH0tqk9Qq6Ym0NE0qL9e6devk84saQK/w6lsCikQiqqysVHp6uvLy8iRJzc3NKisr05QpUxyvQxzjLQHoWy0tLVqzZo3uuOMO11MQn4gS+l5jY6M2b96sm2++2fUUxB/ep4S+FwqFdO211+rVV191PQXoEielJFVfX6/33ntPkyZNcj0F8YOTEvpPdna2ioqKVFZW5noKcBJOSkmupqZGe/fu1ZVXXul6CuzjpIT+l5OTo4suukibNm1yPQWQxEkJXwuHw6qoqFBxcbHrKbCLkxL8M2TIEBUUFGjr1q2upyDJcVLCSQ4dOqTq6mqNGzfO9RTYw0kJ/hs2bJgGDRqkHTt2uJ6CJMVJCV3av3+/WlpaNHr0aNdTYAcnJbhTUFCgYDConTt3up6CJMNJCT365JNPFAgENHLkSNdT4B5/IRc27Nu3T5JUWFjoeAkc48c32FBYWKiOjg5VVla6noIkQJRwRsaMGaOamhpVVFS4noIER5RwxsaPH6/a2lpVVVW5noIERpRwVi699FIdOHBABw8edD0FCYoo4awVFxfriy++0JEjR1xPQQIiSuiV4uJi7d69W9XV1a6nIMEQJfTaNddcoz179qi2ttb1FCQQooRv5eqrr9a2bdtUV1fnegoSBG+eRJ/YuHGjJkyYoKysLNdT0H948yTixw033KANGzaosbHR9RTEOU5K6FOlpaW6/vrrlZmZ6XoK+h4nJcSfyZMn6/XXX1dzc7PrKYhTRAl9burUqSotLVVLS4vrKYhDRAn94vbbb9fq1avV2trqegriDFFCv/A8T9OnT9crr7yitrY213MQR4gS+tW0adO0cuVKtbe3u56COMGrb/DFihUrNH36dKWmprqegt7j1TckjpkzZ+r5559XJBJxPQXGcVKCr5YtW6ZZs2YpJSXF9RScPU5KSDwPPPCAlixZIp+/GSKOcFKCE4sXL9bDDz+sYDDoegrOHCclJK65c+dq0aJFrmfAIKIEJzzP04IFC/TYY48pGo26ngNDiBKcWrhwoUpKSlzPgCFcU4IJ8+bN06JFi+R5nqqqqrTtzTc1fORIjb74Yj5xwA7ukIvkEYvFNH/+fB1dtUq/rqpShr76zfJhIKDNS5fqvtmzXU8EUUIyicViujM/Xy8fPnzKY/slHdu1S2PGjvV/GE5ElJA8Kvfv16DhwzWwm8fvGj5cf+DuvK7xlgAkj88rKroNkiT9kLvyJg2iBBOG5uWpo4fHP+Jid9IgSjDh/OHD9VGg69+OMUn/LirydxCcIUowIT09XTtXrdJb3smXLaKSbh01SmvXr3czDL7jQjdMqQ6HdcPYsfrp0aPalZmppokTtbq01PUsfIVX3wCYwqtvAJIPUQJgClECYApRAmAKUQJgClECYApRAmAKUQJgClECYApRAmAKUQJgClECYApRAmAKUQJgClECYApRAmAKUQJgClECYApRAmAKUQJgClECYApRAmAKUQJgClECYApRAmCK+Sg1NzXpiRkz9P2UFP0gLU1L581Te3u761kA+onp23aHw2FtHDZMP45EOu8XHJP0UEaGljQ0KBgM9v1CAN3x5bbdpqN097BhWllV1eVjk0aO1D/Ky/tkFIAzktxRqq2tVXtOjoZ0s6/c83R+a6vS0tL6bByAHvkSJbPXlI63tmpAD8H8jufp+PHjPi4C4AezURqUk6MjXvdh3hkIKDMz08dFAPxgNkppaWm6Z/z4Lh+LSHrxuuvk9RAtAPHJbJQkacOOHbq1qEhtJ3ytQdKcGTNUun69q1kA+pHZC90n2r59u54rKVHagAF6ZPFiXXjhhX29C8DpJferbwDMSe5X3wAkJ6IEwBSiBMAUogTAFKIEwBSiBMAUogTAFKIEwBSiBMAUogTAFKIEQJFIRJFIxPUMSUQJSGp7PvxQi4cO1Tvp6XonPV1P5eXpoz17nG7iL+QCSWrfvn2qHT1aE77x9W2Scvbt6+rTOPgLuQD6z9+Kik4JkiRdLumvRUV+z+nESQlIQg0NDWo+5xzldfPn/1AgoFBdnUKh0Ilf5qQEoH9Eo9Ee//AHYjFFo1Hf9pz03E6eFYBT2dnZ2t3DZ9zvCgSUlZXl46L/I0pAEvI8T9sff1wNXTzWIOk/JSXObszBNSUgif12+XJNnDVLhV93YK/n6a3lyzXznnu6+uV8RjeA/heJRPTGG2/onOxsFV12mYLBYHe/lCgB8MfBgweVn5+vQKDHKzq8+gbAHx0dHWZu7kqUAKi5uZkoAbCjqanJ9YRORAmAmpubXU/oRJQAcFICYAsnJQCmECUApvDjGwBTOCkBMOXYsWOuJ3QiSgDU0tLiekInogSAkxIAWzgpATBlwIABrid0IkpAkopEIlqxbJmmpKaq9cUX9eDEidr9wQeuZ/F5SkAyikajmhsK6VfNzSd9SFK1pP3vvqvvdn2LJT5PCUD/WLF06SlBkqRzJb121VUuJnUiSkASevvJJ7s99jzd1qaysjJf95yIKAFJaER7e7ePeZIqPv3UvzHffH6uKQHJ54rsbG1t6OoGS9IxSdH6+q7u+8Y1JQD946W331Z9F1+PSZpcUODsRpQSUQKS0thLLtEfn3lGa0/4XO6YpKuGDtUGhz+6Sfz4BiS19vZ2PfXoozpYWal758zRuHHjevrl3PcNgClcUwKQfIgSAFOIEgBTiBIAU4gSAFOIEgBTiBIAU4gSAFOIEgBTiBIAU4gSAFOIEgBTiBIAU4gSAFOIEgBTiBIAU4gSAFOIEgBTiBIAU4gSAFOIEgBTiBIAU4gSAFOIEgBTiBIAU4gSAFOIEgBTiBIAU4gSAFOIEgBTiBIAU4gSAFOIEgBTiBIAU4gSAFOIEgBTiBIAU4gSAFOIEgBTiBIAU4gSAFOIEgBTiBIAU4gSAFOIEgBTiBIAU4gSAFOIEgBTiBIAU4gSAFOIEgBTiBIAU4gSAFOIEgBTiBIAU4gSAFNSXA+QpJqaGr303HOS52nm/fdr8ODBricBcMSLxWJ+Pt8pT/bI7Nl68Nlnlfv1P1dL+s1DD6lk6VI/dwE4Pc+XJ3EZpVtuvFFr1q8/5bjWIelHN92ktevW+bcMwOkkdpRisZi2B4Oa0M3z/8vzdEUkIs/z5b8DgNPz5Q+jswvdtbW1GtlDEEfFYqqvr/dxEQALnEUpNTVV7T2cgo4HAkpJMXEdHoCPnEUpFAppbn5+t4//Ij9fAwcO9HERAAucXugOh8N6Ky9Pt8ZinT+sxiT9xfM0MRxWbm7uqf8GAK4k9oXu/2mor9eTt92mxi1bFJOUPXGi5r/2mrKysvzcBeD0kiNKAOJGYr/6BgBdIUoATCFKAEwhSgBMIUoATCFKAEwhSgBMIUoATCFKAEwhSgBMIUoATCFKAEwhSgBMIUoATCFKAEwhSgBMIUoATCFKAEwhSgBMIUoATCFKQB8Ih8MqLy9XW1ub6ylxjygB30JdXZ3uzcxU1tChOn/UKB1NT9f3Lr/c9ay4xi2WgF5qbGzU07m5Kjl+/JTHJo8bp9L333ewql9x3zfAsgljxuidPXu6fOyIpNjhwzp3yBB/R/Uv7vsGWHb35593+1iupM/Ky33bkkiIEtBLHV7PB4dgaqpPSxILUQJ66U+Fhd0+ViVpxIgR/o1JIEQJ6KVN27frJ+npXT42bcIEDc7J8XlRYiBKQC+lpKRoeTis+847T3uDQR3wPL0ZCGjm1Kn6+7ZtrufFLV59A/pAW1ub2tvblZGRoUAgYb/X85YAAKbwlgAAyYcoATAlxefn8+X4ByB+cVICYApRAmAKUQJgClECYApRAmAKUQJgClECYApRAmAKUQJgClECYApRAmAKUQJgClECYApRAmAKUQJgClECYApRAmAKUQJgClECYApRAmAKUQJgClECYMp/AcYKZtFoYZryAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(figsize=(5, 5))\n",
    "arr_img_graph = plt.imread(\"./imagenes/red.png\", format='png')\n",
    "image_graph = OffsetImage(arr_img_graph, zoom=0.4)\n",
    "image_graph.image.axes = axes\n",
    "ab = AnnotationBbox(image_graph, [0.5, 0.5], frameon=False)\n",
    "axes.add_artist(ab)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulacion(Num_agentes, Num_iteraciones, UMBRAL, inicial, N, PARS):\n",
    "\n",
    "    #agentes = crear_agentes_aleatorios(Num_agentes)\n",
    "    agentes = F.crear_agentes1()\n",
    "    politicas = crear_politicas()\n",
    "\n",
    "\n",
    "    # Leyendo red de archivo\n",
    "    F.leer_red(agentes)\n",
    "\n",
    "    for i in range(Num_iteraciones):\n",
    "        agentes = F.juega_ronda(agentes, politicas, UMBRAL)\n",
    "        agentes = agentes_aprenden(agentes, i)\n",
    "\n",
    "    data = F.crea_dataframe_agentes(agentes, Num_iteraciones, PARS, N)\n",
    "    data['Politica_lag'] = data.groupby('Agente')['Politica'].transform('shift', 1)\n",
    "    data['Consistencia'] = data.apply(lambda x : F.encontrar_consistencia (x['Politica'], x['Politica_lag']), axis=1)\n",
    "    F.guardar(data, 'agentes.csv', inicial)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correr simulación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identificador = 0\n",
    "UMBRAL = 0.5\n",
    "inicial = True\n",
    "simulacion(Num_agentes, Num_iteraciones, UMBRAL, inicial, identificador, PARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comando para leer a partir de datos\n",
    "data = pd.read_csv('data/agentes.csv')\n",
    "print(\"Numero de experimentos:\", len(data.Identificador.unique()))\n",
    "print(data.groupby('Identificador').Agente.value_counts())\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Asistencia por ronda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(data.groupby('Ronda')['Estado'].mean())\n",
    "df1.columns = ['Asistencia']\n",
    "df1['Asistencia'] = df1['Asistencia'] * 100\n",
    "plt.ylabel('Asistencia (%)')\n",
    "plt.ylim(0,100)\n",
    "plt.title('Asistencia a El Farol por ronda')\n",
    "df1['Asistencia'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Puntaje promedio que obtuvo cada política vs. ronda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data.groupby(['Ronda','Politica'])['Puntaje'].mean())\\\n",
    "                .reset_index()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for label,group in df.groupby('Politica'):\n",
    "    group.plot(x='Ronda', y='Puntaje', ax=ax,label=label)\n",
    "plt.ylabel(\"Puntaje Promedio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uso Politicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data.groupby(['Ronda','Politica'])['Agente'].count()).reset_index()\n",
    "df.columns = ['Ronda', 'Politica', 'num_agentes']\n",
    "uso_politica = pd.pivot_table(\n",
    "    data=df,\n",
    "    index='Ronda',\n",
    "    values='num_agentes',\n",
    "    columns='Politica',\n",
    "    aggfunc='sum'\n",
    ").reset_index()\n",
    "\n",
    "uso_politica = uso_politica.sort_values(by='Ronda',ascending=False)\n",
    "uso_politica = uso_politica.set_index('Ronda')\n",
    "ax = sns.heatmap(uso_politica)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize = (12,5))\n",
    "for label,group in df.groupby('Politica'):\n",
    "    group.plot(x='Ronda', y='num_agentes', ax=ax[0],label=label)\n",
    "ax[0].set_ylim([0,Num_agentes+1])\n",
    "ax[0].set_title('Uso de política por ronda')\n",
    "ax[0].set_ylabel('Número de agentes')\n",
    "df1 = pd.DataFrame(data.groupby('Politica')['Agente'].count())\n",
    "df1.columns = ['Agentes']\n",
    "#ax[1].plot(df.index, df['Agentes'])\n",
    "df1.plot(kind=\"bar\", ax = ax[1])\n",
    "ax[1].set_title('Uso total de políticas')\n",
    "ax[1].set_ylabel('Cantidad total de agentes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribución de la recompensa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(x):\n",
    "        # The rest of the code requires numpy arrays.\n",
    "        x = np.asarray(x) #convierte x en un vector\n",
    "        sorted_x = np.sort(x) #los organiza de menor a mayor\n",
    "        n = len(x) #tamaño del vector x\n",
    "        cumx = np.cumsum(sorted_x, dtype=float)# va haciendo la suma acumulativa\n",
    "        gini = (n + 1 - 2 * np.sum(cumx) / cumx[-1]) / n #esta es una formula muy rara, esta en alternate expression de Wikipedia\n",
    "        cumx = cumx/cumx[-1] #normalizando para que se vea en la grafica bien\n",
    "        cumx = [0] + list(cumx)\n",
    "        pe_line = np.linspace(start=0.0, stop=1.0, num=len(cumx))\n",
    "        # The above formula, with all weights equal to 1 simplifies to:\n",
    "        return gini, pe_line, cumx\n",
    "\n",
    "data_aux = data.copy()\n",
    "data_aux['Puntaje normalizado'] = data_aux['Puntaje'] + 1\n",
    "data_aux = data_aux.groupby('Agente')['Puntaje normalizado'].sum().reset_index()\n",
    "recompensa = data_aux['Puntaje normalizado'].values\n",
    "gini, pe_line,cumx = gini(recompensa)\n",
    "plt.plot(pe_line, cumx, label='Curva de Lorenz', marker=\"o\")\n",
    "plt.plot(pe_line, pe_line, label='Igualdad perfecta')\n",
    "plt.fill_between(pe_line, cumx)\n",
    "plt.title('Gini: {}'.format(gini), fontsize=12)\n",
    "plt.ylabel('Porcentaje de acumulación', fontsize=10)\n",
    "plt.xlabel('Quintiles de ingreso (menor a mayor)', fontsize=10)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
