\documentclass[amsmath,amssymb]{article}      
                                                                                
%\usepackage{dcolumn}%                                                          
\usepackage{graphicx}%  needed for figures                                      
\usepackage{amsmath}% needed for \tfrac, \bmatrix, etc.                         
\usepackage{amsfonts}% needed for bold Greek, Fraktur, and blackboard bold need 
\usepackage{bm}%                                                                
\usepackage{epsfig}%                                                            
\usepackage{multirow}%                                                          
%\usepackage{dsfont}                                                            
\usepackage[utf8x]{inputenc}

\usepackage[spanish]{babel}
\usepackage{color}                                                              
%\usepackage{bbm}                                                               
\usepackage{boldline}                                                           
\usepackage{mathtools}
\usepackage{caption}
\usepackage{diagbox}
\usepackage{capt-of}
\usepackage[normalem]{ulem}    

\newcommand{\D}[1]{\frac{\partial}{\partial{#1}}}                               
\newcommand{\Dd}[2]{\frac{\partial{#1}}{\partial{#2}}}                          
\newcommand{\deriv}[1]{\frac{d}{d{#1}}}                                         
\newcommand{\derivd}[2]{\frac{d{#1}}{d{#2}}}                                    
\newcommand{\indicator}{1\!\!1}                                                 
\newcommand*{\medcap}{\mathbin{\scalebox{1.5}{\ensuremath{\cap}}}}%             
\newcommand*{\Tau}{\mathcal{T}}                                                 
\newcommand{\myeq}[1]{\stackrel{\mathclap{\normalfont\mbox{#1}}}{=}}

\DeclareMathOperator\erf{erf}                                                   
\DeclareMathOperator\erfc{erfc}                                                 
                                                                                
\newcommand{\comm}[1]{{\color{magenta}{#1}}}                                    
\newcommand{\newt}[1]{{\color{blue}{#1}}}                                       
\newcommand{\somath}[1]{\hbox{\sout{$#1$}}}                                     
                                                                                
\begin{document}                                                                
\title{Modelo}
\author{}                                      
\date{\today}
\maketitle

El modelo actual pretende describir la dinámica de un grupo de $N$ agentes (pueden representar personas) que interactúan en un medio (puede representar el bar) para obtener ganancias en un {\it puntaje} $p$ (puede representar satisfacción). La dinámica se modela como una serie de rondas indexadas por el índice $t$, en cada una de las cuales cada agente realiza la acción de interactuar o no con el medio (asistir o no al bar). El puntaje que recibe cada agente al final de la ronda depende de si interactuó o no con el medio, a lo que se denominará {\it estrategia} ($s$), y de la fracción de individuos que decidieron interactuar ($\rho=n/N$) en esa ronda, en donde $n$ representa el número de individuos que decidieron interactuar.\\

La estrategia $s$ puede tomar los valores 1 o 0 (interactuar o no interactuar), mientras que el puntaje asignado a un agente se determina de la siguiente manera:
\begin{itemize}
\item Si el agente decide no interactuar ($s=0$) su puntaje es $p=0$
\item Si el agente decide interactuar ($s=1$) y en el medio $\rho>R$, su puntaje es $p=-1$
\item Si el agente decide interactuar ($s=1$) y en el medio $\rho\leq R$, su puntaje es $p=1$
\end{itemize}
en donde $0\leq R\leq 1$ representa el {\it umbral} de la interacción. De ésta manera, para el agente $i$ en la ronda $t$ tenemos
\begin{align}
  p_t^{(i)}=\left\{
  \begin{array}{cl}
    0 & \mbox{, }s_{t}^{(i)}=0\\
    -1 & \mbox{, }s_{t}^{(i)}=1 \mbox{ and }\rho_t>R\\
    1 & \mbox{, }s_{t}^{(i)}=1 \mbox{ and }\rho_t\leq R\\
  \end{array}
  \right. .
\end{align}

La estrategia que adopta el agente $i$ durante una ronda $t$ está determinada por una regla que depende de la estrategia que adoptó la ronda anterior ($s_{t-1}^{(i)}$) y del puntaje que obtuvo en la ronda anterior ($p_{t-1}^{(i)}$)
\begin{align}
  s_t^{(i)}=f_t^{(i)}(s_{t-1}^{(i)},p_{t-1}^{(i)}).
\end{align}
A la función $f_t^{(i)}$ la llamaremos la {\it política} del agente $i$ en la ronda $t$. En el modelo actual se definen 8 políticas distintas:\\
\begin{table}
\begin{minipage}{0.5\textwidth}
  \centering
  \captionof*{table}{Política 0}
  \begin{tabular}{|l||*{3}{c|}}
    \hline
    \diagbox{$s_{t-1}$}{$p_{t-1}$} & -1 & 0 & 1\\
    \hline\hline
    0 & $\times$ & 0 & $\times$\\
    \hline
    1 & 0 & $\times$ & 0\\
    \hline
  \end{tabular}
\end{minipage}
\vspace{0.7cm}
\begin{minipage}{0.5\textwidth}
  \centering
  \captionof*{table}{Política 1}
  \begin{tabular}{|l||*{3}{c|}}
    \hline
    \diagbox{$s_{t-1}$}{$p_{t-1}$} & -1 & 0 & 1\\
    \hline\hline
    0 & $\times$ & 0 & $\times$\\
    \hline
    1 & 0 & $\times$ & 1\\
    \hline
  \end{tabular}
\end{minipage}
\begin{minipage}{0.5\textwidth}
  \centering
  \captionof*{table}{Política 2}
  \begin{tabular}{|l||*{3}{c|}}
    \hline
    \diagbox{$s_{t-1}$}{$p_{t-1}$} & -1 & 0 & 1\\
    \hline\hline
    0 & $\times$ & 0 & $\times$\\
    \hline
    1 & 1 & $\times$ & 0\\
    \hline
  \end{tabular}
\end{minipage}
\vspace{0.7cm}
\begin{minipage}{0.5\textwidth}
  \centering
  \captionof*{table}{Política 3}
  \begin{tabular}{|l||*{3}{c|}}
    \hline
    \diagbox{$s_{t-1}$}{$p_{t-1}$} & -1 & 0 & 1\\
    \hline\hline
    0 & $\times$ & 0 & $\times$\\
    \hline
    1 & 1 & $\times$ & 1\\
    \hline
  \end{tabular}
\end{minipage}
\begin{minipage}{0.5\textwidth}
  \centering
  \captionof*{table}{Política 4}
  \begin{tabular}{|l||*{3}{c|}}
    \hline
    \diagbox{$s_{t-1}$}{$p_{t-1}$} & -1 & 0 & 1\\
    \hline\hline
    0 & $\times$ & 1 & $\times$\\
    \hline
    1 & 0 & $\times$ & 0\\
    \hline
  \end{tabular}
\end{minipage}
\vspace{0.7cm}
\begin{minipage}{0.5\textwidth}
  \centering
  \captionof*{table}{Política 5}
  \begin{tabular}{|l||*{3}{c|}}
    \hline
    \diagbox{$s_{t-1}$}{$p_{t-1}$} & -1 & 0 & 1\\
    \hline\hline
    0 & $\times$ & 1 & $\times$\\
    \hline
    1 & 0 & $\times$ & 1\\
    \hline
  \end{tabular}
\end{minipage}
\begin{minipage}{0.5\textwidth}
  \centering
  \captionof*{table}{Política 6}
  \begin{tabular}{|l||*{3}{c|}}
    \hline
    \diagbox{$s_{t-1}$}{$p_{t-1}$} & -1 & 0 & 1\\
    \hline\hline
    0 & $\times$ & 1 & $\times$\\
    \hline
    1 & 1 & $\times$ & 0\\
    \hline
  \end{tabular}
\end{minipage}
\begin{minipage}{0.5\textwidth}
  \centering
  \captionof*{table}{Política 7}
  \begin{tabular}{|l||*{3}{c|}}
    \hline
    \diagbox{$s_{t-1}$}{$p_{t-1}$} & -1 & 0 & 1\\
    \hline\hline
    0 & $\times$ & 1 & $\times$\\
    \hline
    1 & 1 & $\times$ & 1\\
    \hline
  \end{tabular}
\end{minipage}\\
\caption{Valores para $s_t$ dados por $f_t(s_{t-1},p_{t-1})$ según la política.}
\end{table}

Adicionalmente se introduce en el modelo la posibilidad de que un agente cambie su política de una ronda a la siguiente. A éste proceso lo llamaremos {\it aprendizaje}. La regla de aprendizaje usada es la siguiente: Al final de cada ronda, cada agente adopta la política del agente vecino que obtenga el mayor puntaje. Para definir los vecinos de un agente, definimos un grafo en el que los nodos son los agentes y éstos se encuentran conectados por aristas. Definimos la {\it vecindad} de un agente $i$ como todos los agentes $j\in 1,2,3\cdots N$ que comparten una arista con $i$ (incluyéndolo a él mismo). Si el agente $i$ tiene el puntaje máximo, mantiene su política. Si el agente $i$ no tiene el puntaje máximo, pero hay más de un agente en su vecindad con el puntaje máximo, se copia la política de uno de estos agentes escogido aleatoriamente.\\

\section*{Simulación}
Una simulación de la dinámica del sistema consiste en los siguientes pasos:
\begin{enumerate}
\item Se escoge un valor para $R$
\item Se crean $N$ agentes, con $p_0^{(i)}=0$ y $s_0^{(i)}$ escogido aleatoriamente de entre los valores -1, 0 y 1. Igualmente a cada agente se le asigna una política inicial $f_0^{(i)}$ de manera aleatoria
\item Se genera el grafo que representa la vecindad entre los agentes
\item Se suma el número de agentes $n_t$ que interactuaron, (con estrategia $s_0^{(i)}=1$), se calcula $\rho_t=n_t/N$ y se compara con $R$
\item
  \begin{itemize}
  \item Se asigna $p_t^{(i)}=0$ todos los agentes $i$ con la estrategia $s_t^{(i)}=0$
  \item Si $\rho_t\leq R$ se asigna $p_t^{(i)}=1$ a todos los agentes con la estrategia $s_t^{(i)}=1$
  \item Si $\rho_t>R$ se asigna $p_t^{(i)}=-1$ a todos los agentes con la estrategia $s_t^{(i)}=1$
  \end{itemize}
\item Para cada agente, se compara su puntaje con el de sus vecinos. Si hay vecinos con puntaje mayor al suyo, se reemplaza la política actual por la de uno de los agentes del conjunto de vecinos con puntaje mayor, escogido de manera aleatoria
\item Cada agente actualiza su estrategia $s_{t+1}^{(i)}$ de acuerdo a su nueva política $f_t^{(i)}(s_{t}^{(i)},p_{t}^{(i)})$
\item Se continúa con el paso 4.
\end{enumerate}
     

\end{document}
